{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face x LangChain : A new partner package in LangChain\n",
    "langchain_huggingface, a partner package in LangChain jointly maintained by Hugging Face and LangChain. This new Python package is designed to bring the power of the latest development of Hugging Face into LangChain and keep it up to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_huggingface in e:\\udemy final\\langchain\\venv\\lib\\site-packages (0.0.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from langchain_huggingface) (0.23.4)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.52 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from langchain_huggingface) (0.2.10)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from langchain_huggingface) (3.0.1)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from langchain_huggingface) (0.19.1)\n",
      "Requirement already satisfied: transformers>=4.39.0 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from langchain_huggingface) (4.41.2)\n",
      "Requirement already satisfied: filelock in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (6.0.1)\n",
      "Requirement already satisfied: requests in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (0.1.77)\n",
      "Requirement already satisfied: pydantic<3,>=1 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (1.10.13)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (8.4.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.3.1)\n",
      "Requirement already satisfied: numpy in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.5.0)\n",
      "Requirement already satisfied: scipy in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
      "Requirement already satisfied: Pillow in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (10.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from transformers>=4.39.0->langchain_huggingface) (2024.5.15)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from transformers>=4.39.0->langchain_huggingface) (0.4.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain_huggingface) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_huggingface) (3.10.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2024.6.2)\n",
      "Requirement already satisfied: sympy in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.12.1)\n",
      "Requirement already satisfied: networkx in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.3)\n",
      "Requirement already satisfied: jinja2 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.4)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2021.4.0)\n",
      "Requirement already satisfied: colorama in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.0->langchain_huggingface) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2021.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in e:\\udemy final\\langchain\\venv\\lib\\site-packages (0.23.4)\n",
      "Requirement already satisfied: filelock in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from huggingface_hub) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from huggingface_hub) (2024.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from huggingface_hub) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: colorama in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from requests->huggingface_hub) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\udemy final\\langchain\\venv\\lib\\site-packages (from requests->huggingface_hub) (2024.6.2)\n"
     ]
    }
   ],
   "source": [
    "## API Call\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFaceEndpoint\n",
    "#### How to Access HuggingFace Models with API\n",
    "There are also two ways to use this class. You can specify the model with the repo_id parameter. Those endpoints use the serverless API, which is particularly beneficial to people using pro accounts or enterprise hub. Still, regular users can already have access to a fair amount of request by connecting with their HF token in the environment where they are executing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n",
      "/Users/mdashikadnan/Documents/adnanedu/python/langchain/genai-langchain/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/mdashikadnan/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.3', temperature=0.7, stop_sequences=[], server_kwargs={}, model_kwargs={'max_length': 150, 'token': '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'}, model='mistralai/Mistral-7B-Instruct-v0.3', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'? Machine learning is a subset of artificial intelligence that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. It focuses on the development of computer programs that can access data and use it to learn for themselves.\\n\\nMachine learning is a way of achieving artificial intelligence by training computer systems to perform tasks without being explicitly programmed to perform them. Machine learning algorithms use statistical methods to automatically improve their performance on a specific task by learning from and making predictions or decisions based on data.\\n\\nThere are several types of machine learning, including supervised learning, unsupervised learning, and reinforcement learning. In supervised learning, the algorithm is trained on a labeled dataset, where the correct output is provided for each input. In unsupervised learning, the algorithm is given an unlabeled dataset and must find patterns and structure on its own. Reinforcement learning involves an agent learning to take actions in an environment to maximize a reward signal.\\n\\nMachine learning is used in a wide variety of applications, including image and speech recognition, natural language processing, recommendation systems, and autonomous vehicles. It has the potential to revolutionize many industries, including healthcare, finance, and retail, by enabling computers to make decisions and predictions based on large amounts of data.\\n\\nMachine learning is a powerful tool for analyzing and making predictions based on data, but it also has limitations. It requires large amounts of high-quality data to train algorithms, and it can be difficult to interpret the results of machine learning models. Additionally, machine learning models can be vulnerable to bias and errors, and they may not always perform well in real-world settings. Despite these challenges, machine learning is a rapidly growing field with many exciting applications and potential for further development.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ðŸ’¡\\n\\nGenerative AI is a type of artificial intelligence that creates new content by learning patterns from existing data. It uses machine learning algorithms to generate data that is similar to the data it was trained on, but is unique and not a direct copy.\\n\\nThere are several types of generative AI, including:\\n\\n1. Generative Adversarial Networks (GANs): These are a class of algorithms used for generative tasks, such as generating images, music, and text. They consist of two neural networks: a generator network that creates new data, and a discriminator network that tries to distinguish between the new data and the real data.\\n2. Recurrent Neural Networks (RNNs): These are a type of artificial neural network that can learn from sequences of data, such as text or speech. They are often used for tasks such as language translation, speech recognition, and text generation.\\n3. Variational Autoencoders (VAEs): These are a type of generative model that can learn to represent data in a compact form, such as a lower-dimensional space. They are often used for tasks such as image generation, text generation, and anomaly detection.\\n4. Transformers: These are a type of model architecture that can handle long sequences of data, such as text or speech. They are often used for tasks such as language translation, text summarization, and text generation.\\n\\nGenerative AI has many potential applications, including:\\n\\n* Creating new art, music, and other forms of creative content\\n* Generating synthetic data for training other AI models\\n* Creating personalized content for users, such as personalized recommendations or personalized news articles\\n* Generating realistic simulations for video games or virtual reality experiences\\n* Generating text for chatbots or other conversational AI systems\\n* Generating code for software development\\n* Generating scientific data for research purposes\\n\\nOverall, generative AI is a powerful tool that has the potential to revolutionize many industries and applications. However, it also raises important ethical and societal questions, such as the potential for AI to create fake news, deepfakes, and other forms of misinformation, and the potential for AI to replace human jobs in creative industries.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is generative AI \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/mdashikadnan/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='google/gemma-2-9b', temperature=0.7, stop_sequences=[], server_kwargs={}, model_kwargs={'max_length': 150, 'token': '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'}, model='google/gemma-2-9b', client=<InferenceClient(model='google/gemma-2-9b', timeout=120)>, async_client=<InferenceClient(model='google/gemma-2-9b', timeout=120)>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"google/gemma-2-9b\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "(Request ID: IRlMD5A0FPUy_4MfABbWl)\n\n403 Forbidden: None.\nCannot access content at: https://api-inference.huggingface.co/models/google/gemma-2-9b.\nMake sure your token has the correct permissions.\nThe model google/gemma-2-9b is too large to be loaded automatically (36GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/adnanedu/python/langchain/genai-langchain/venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/adnanedu/python/langchain/genai-langchain/venv/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://api-inference.huggingface.co/models/google/gemma-2-9b",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is machine learning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/adnanedu/python/langchain/genai-langchain/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py:391\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    388\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    389\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 391\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    402\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    403\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/adnanedu/python/langchain/genai-langchain/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py:756\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    750\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    754\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    755\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 756\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/adnanedu/python/langchain/genai-langchain/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py:950\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    936\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    937\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    938\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    948\u001b[0m         )\n\u001b[1;32m    949\u001b[0m     ]\n\u001b[0;32m--> 950\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/adnanedu/python/langchain/genai-langchain/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py:793\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    792\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 793\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    794\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/Documents/adnanedu/python/langchain/genai-langchain/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py:780\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    771\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    772\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    777\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    779\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 780\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    784\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    788\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    789\u001b[0m         )\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Documents/adnanedu/python/langchain/genai-langchain/venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py:1514\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1511\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m   1513\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1514\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1515\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1516\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1517\u001b[0m     )\n\u001b[1;32m   1518\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/Documents/adnanedu/python/langchain/genai-langchain/venv/lib/python3.10/site-packages/langchain_huggingface/llms/huggingface_endpoint.py:288\u001b[0m, in \u001b[0;36mHuggingFaceEndpoint._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m     invocation_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m invocation_params[\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     ]  \u001b[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minvocation_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001b[39;00m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/adnanedu/python/langchain/genai-langchain/venv/lib/python3.10/site-packages/huggingface_hub/inference/_client.py:305\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 305\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m~/Documents/adnanedu/python/langchain/genai-langchain/venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:468\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m    463\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure your token has the correct permissions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    467\u001b[0m     )\n\u001b[0;32m--> 468\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m416\u001b[39m:\n\u001b[1;32m    471\u001b[0m     range_header \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRange\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: (Request ID: IRlMD5A0FPUy_4MfABbWl)\n\n403 Forbidden: None.\nCannot access content at: https://api-inference.huggingface.co/models/google/gemma-2-9b.\nMake sure your token has the correct permissions.\nThe model google/gemma-2-9b is too large to be loaded automatically (36GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints)."
     ]
    }
   ],
   "source": [
    "llm.invoke(\"What is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/mdashikadnan/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.3', temperature=0.7, stop_sequences=[], server_kwargs={}, model_kwargs={'max_length': 150, 'token': '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'}, model='mistralai/Mistral-7B-Instruct-v0.3', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] input_types={} partial_variables={} template='\\nQuestion:{question}\\nAnswer:Lets think step by step.\\n'\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate,LLMChain\n",
    "template=\"\"\"\n",
    "Question:{question}\n",
    "Answer:Lets think step by step.\n",
    "\"\"\"\n",
    "prompt=PromptTemplate(template=template,input_variables=[\"question\"])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g5/58p5c8f568398j2qrhdc7smw0000gn/T/ipykernel_64963/3735178259.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  llm_chain=LLMChain(llm=llm,prompt=prompt)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'? India\\n\\nIndia won the Cricket World Cup 2011 after defeating Sri Lanka in the final match held on April 2, 2011, at the Wankhede Stadium in Mumbai, India. India won the toss and elected to bat first, scoring 277 runs in 50 overs, with Gautam Gambhir scoring 97 runs and Mahendra Singh Dhoni scoring an unbeaten 91 runs. In response, Sri Lanka struggled to keep up with the run rate and were all out for 275 runs in 48.4 overs. Yuvraj Singh was named the Man of the Tournament for his all-round performances throughout the tournament.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain=LLMChain(llm=llm,prompt=prompt)\n",
    "llm.invoke(\"Who won the cricket World up on 2011?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdashikadnan/Documents/adnanedu/python/langchain/genai-langchain/venv/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPIEmbeddings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = hf.embed_query(\"hi this is harrison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.028416534885764122,\n",
       " 0.01218326948583126,\n",
       " 0.02744394913315773,\n",
       " -0.05482875555753708,\n",
       " 0.02423890121281147,\n",
       " 0.0007662660791538656,\n",
       " 0.06783360987901688,\n",
       " 0.016348334029316902,\n",
       " -0.01895071379840374,\n",
       " 0.012542922049760818,\n",
       " 0.02156498096883297,\n",
       " -0.08793039619922638,\n",
       " 0.0006460822769440711,\n",
       " 0.033270854502916336,\n",
       " 0.005463770590722561,\n",
       " -0.06037646159529686,\n",
       " 0.05042261257767677,\n",
       " 0.004434832837432623,\n",
       " 0.0009598866454325616,\n",
       " 0.0017405245453119278,\n",
       " 0.003298781346529722,\n",
       " 0.03167248144745827,\n",
       " -0.048807475715875626,\n",
       " -0.04481916502118111,\n",
       " 0.07132108509540558,\n",
       " -0.00751086138188839,\n",
       " -0.0011259402381256223,\n",
       " -0.0158012006431818,\n",
       " -0.029402343556284904,\n",
       " -0.17224563658237457,\n",
       " -0.03189520165324211,\n",
       " -0.0016291036736220121,\n",
       " 0.018105003982782364,\n",
       " 0.015315421856939793,\n",
       " -0.02072957530617714,\n",
       " -0.008872942067682743,\n",
       " -0.0012822344433516264,\n",
       " 0.027276882901787758,\n",
       " -0.010114283300936222,\n",
       " 0.012621619738638401,\n",
       " -0.007077816408127546,\n",
       " -0.01669319160282612,\n",
       " 0.04085583612322807,\n",
       " 0.023938341066241264,\n",
       " -0.020081548020243645,\n",
       " 0.028681201860308647,\n",
       " -0.019400721415877342,\n",
       " -0.014618170447647572,\n",
       " 0.017379608005285263,\n",
       " 0.0041640931740403175,\n",
       " 0.06415645033121109,\n",
       " 0.047683075070381165,\n",
       " 0.0018365092109888792,\n",
       " -8.067922317422926e-05,\n",
       " 0.016596786677837372,\n",
       " 0.01112419180572033,\n",
       " 0.06969434022903442,\n",
       " 0.05182048678398132,\n",
       " 0.05568530037999153,\n",
       " 0.05551541596651077,\n",
       " 0.0005039346870034933,\n",
       " 0.04187058284878731,\n",
       " -0.153440922498703,\n",
       " 0.05180785432457924,\n",
       " 0.006689800880849361,\n",
       " -0.031670697033405304,\n",
       " -0.009104994125664234,\n",
       " -0.05160469189286232,\n",
       " 0.0425085723400116,\n",
       " 0.028200045228004456,\n",
       " -0.010748148895800114,\n",
       " 0.022405827417969704,\n",
       " 0.044395532459020615,\n",
       " 0.004115530755370855,\n",
       " 0.018998486921191216,\n",
       " -0.0043571824207901955,\n",
       " 0.04762765020132065,\n",
       " 0.011824642308056355,\n",
       " 0.00816459022462368,\n",
       " 0.008177240379154682,\n",
       " -0.009698738344013691,\n",
       " -0.01426029298454523,\n",
       " 0.011409678496420383,\n",
       " -0.07362115383148193,\n",
       " -0.05439525097608566,\n",
       " -0.057039640843868256,\n",
       " -0.003608550876379013,\n",
       " 0.0026661320589482784,\n",
       " 0.023782506585121155,\n",
       " 0.01537622231990099,\n",
       " -0.0702037513256073,\n",
       " -0.0313003771007061,\n",
       " -0.0031142488587647676,\n",
       " -0.01581217162311077,\n",
       " -0.03791400045156479,\n",
       " -0.025921881198883057,\n",
       " 0.018168453127145767,\n",
       " -0.03882456570863724,\n",
       " -0.056745100766420364,\n",
       " 0.5792059898376465,\n",
       " -0.05278833955526352,\n",
       " 0.02071637287735939,\n",
       " 0.06794385612010956,\n",
       " -0.04541652649641037,\n",
       " 0.01164240576326847,\n",
       " -0.02157173492014408,\n",
       " 0.020341722294688225,\n",
       " -0.02744896151125431,\n",
       " -0.045588936656713486,\n",
       " -0.029443567618727684,\n",
       " -0.02366248331964016,\n",
       " -0.034315258264541626,\n",
       " 0.001938817324116826,\n",
       " -0.07095134258270264,\n",
       " 0.03455634042620659,\n",
       " -0.030558912083506584,\n",
       " 0.039078645408153534,\n",
       " -0.02970728650689125,\n",
       " -0.0008283215574920177,\n",
       " -0.012159406207501888,\n",
       " -0.018272802233695984,\n",
       " 0.02548651024699211,\n",
       " -0.0044616954401135445,\n",
       " 0.016335295513272285,\n",
       " 0.019126486033201218,\n",
       " -0.0548320934176445,\n",
       " 0.02763596549630165,\n",
       " -0.004757663235068321,\n",
       " 0.059001725167036057,\n",
       " -0.001694472273811698,\n",
       " 0.008014949969947338,\n",
       " -0.03772685304284096,\n",
       " -0.09893041104078293,\n",
       " -0.0225744117051363,\n",
       " -0.03760470822453499,\n",
       " -0.0021698439959436655,\n",
       " 0.003244623774662614,\n",
       " -0.019202569499611855,\n",
       " -0.008631163276731968,\n",
       " -0.04802306368947029,\n",
       " 0.008696692995727062,\n",
       " -0.09516109526157379,\n",
       " -0.034960489720106125,\n",
       " -0.043608006089925766,\n",
       " -0.0003439935971982777,\n",
       " -0.010173615999519825,\n",
       " -0.030999520793557167,\n",
       " 0.024309705942869186,\n",
       " -0.02040199190378189,\n",
       " 0.03113936260342598,\n",
       " 0.0008811252773739398,\n",
       " 0.013916514813899994,\n",
       " -0.031196245923638344,\n",
       " -0.037154003977775574,\n",
       " 0.0040296521037817,\n",
       " 0.014799806289374828,\n",
       " 0.043188925832509995,\n",
       " 0.038754843175411224,\n",
       " 0.013851942494511604,\n",
       " 0.019797883927822113,\n",
       " 0.010267063044011593,\n",
       " -0.00543409027159214,\n",
       " -0.014299202710390091,\n",
       " 0.027637841179966927,\n",
       " 0.009802660904824734,\n",
       " -0.1355028599500656,\n",
       " -0.017139768227934837,\n",
       " 0.017617110162973404,\n",
       " 0.023132244125008583,\n",
       " 0.0017590412171557546,\n",
       " 0.030889438465237617,\n",
       " 0.039918702095746994,\n",
       " -0.013684144243597984,\n",
       " 0.024816548451781273,\n",
       " 0.05405019223690033,\n",
       " 0.017761148512363434,\n",
       " -0.01847507245838642,\n",
       " 0.02595536783337593,\n",
       " -0.006377558223903179,\n",
       " -0.016587290912866592,\n",
       " 0.03784799203276634,\n",
       " -0.02729007415473461,\n",
       " -0.052845750004053116,\n",
       " -0.03803321719169617,\n",
       " 0.051911111921072006,\n",
       " -0.007557081989943981,\n",
       " -0.03180529549717903,\n",
       " 0.01328424084931612,\n",
       " -0.027723750099539757,\n",
       " 0.05630653351545334,\n",
       " 0.0030418303795158863,\n",
       " 0.05332479998469353,\n",
       " -0.05791124701499939,\n",
       " -0.011325770057737827,\n",
       " -0.031172029674053192,\n",
       " 0.025608686730265617,\n",
       " 0.033890627324581146,\n",
       " -0.0010284853633493185,\n",
       " 0.015864888206124306,\n",
       " 0.010595173574984074,\n",
       " -0.027037806808948517,\n",
       " -0.0009308183216489851,\n",
       " -0.048152219504117966,\n",
       " 0.02817922830581665,\n",
       " 0.010320593602955341,\n",
       " 0.06662958115339279,\n",
       " -0.016558196395635605,\n",
       " -0.004431385546922684,\n",
       " 0.03823426738381386,\n",
       " -0.023408200591802597,\n",
       " -0.03558177500963211,\n",
       " -0.05829072743654251,\n",
       " -0.011181527748703957,\n",
       " -0.01768452860414982,\n",
       " -0.016141289845108986,\n",
       " -0.034245382994413376,\n",
       " -0.025139516219496727,\n",
       " 0.039396632462739944,\n",
       " -0.023658202961087227,\n",
       " -0.007725039962679148,\n",
       " -0.005098952446132898,\n",
       " -0.03523438796401024,\n",
       " -0.014076842926442623,\n",
       " -0.22326035797595978,\n",
       " -0.03147134557366371,\n",
       " -0.0012905917828902602,\n",
       " -0.0017200229922309518,\n",
       " -0.007846023887395859,\n",
       " -0.05802329257130623,\n",
       " 0.046174556016922,\n",
       " 0.024552645161747932,\n",
       " 0.07320839911699295,\n",
       " 0.017268307507038116,\n",
       " 0.047612112015485764,\n",
       " 0.013473305851221085,\n",
       " -0.005516048055142164,\n",
       " -0.014357827603816986,\n",
       " -0.00967430230230093,\n",
       " 0.04878246411681175,\n",
       " 0.030538123100996017,\n",
       " -0.024993928149342537,\n",
       " 0.02148626185953617,\n",
       " 0.01763981394469738,\n",
       " 0.05313890799880028,\n",
       " 0.013484987430274487,\n",
       " -0.02322598546743393,\n",
       " -0.021403973922133446,\n",
       " 0.026075346395373344,\n",
       " 0.002029192401096225,\n",
       " 0.12753742933273315,\n",
       " 0.08316836506128311,\n",
       " 0.04408947750926018,\n",
       " -0.0267036035656929,\n",
       " 0.005521990824490786,\n",
       " -0.009294873103499413,\n",
       " 0.02007432095706463,\n",
       " -0.09684177488088608,\n",
       " -0.024703940376639366,\n",
       " 0.025086918845772743,\n",
       " 0.002088623819872737,\n",
       " -0.0448940210044384,\n",
       " -0.0786113515496254,\n",
       " -0.004376326221972704,\n",
       " -0.06590452045202255,\n",
       " 0.014689379371702671,\n",
       " -0.05764184892177582,\n",
       " -0.07152023166418076,\n",
       " -0.06232650578022003,\n",
       " 0.0034316496457904577,\n",
       " -0.0460655651986599,\n",
       " 0.04530087485909462,\n",
       " -0.026762235909700394,\n",
       " 0.034010935574769974,\n",
       " 0.04547378793358803,\n",
       " -0.02817927859723568,\n",
       " 0.005011813715100288,\n",
       " 0.009630859829485416,\n",
       " -0.03030557371675968,\n",
       " -0.03612484037876129,\n",
       " -0.013626945205032825,\n",
       " -0.03265369310975075,\n",
       " -0.044677503407001495,\n",
       " 0.010642211884260178,\n",
       " -0.027486301958560944,\n",
       " -0.024565046653151512,\n",
       " -0.024747783318161964,\n",
       " 0.05361957103013992,\n",
       " 0.020789960399270058,\n",
       " 0.01946847513318062,\n",
       " 0.05324121192097664,\n",
       " -0.01400248147547245,\n",
       " 0.021243277937173843,\n",
       " -0.049573224037885666,\n",
       " -0.008522589690983295,\n",
       " 0.007852837443351746,\n",
       " -0.05719399079680443,\n",
       " -0.02755063772201538,\n",
       " 0.005300892982631922,\n",
       " 0.040072910487651825,\n",
       " 0.019597897306084633,\n",
       " -0.045197367668151855,\n",
       " 0.03243589401245117,\n",
       " -0.012342420406639576,\n",
       " 0.034314416348934174,\n",
       " 0.021102115511894226,\n",
       " 0.03984646126627922,\n",
       " 0.031663794070482254,\n",
       " -0.03359024599194527,\n",
       " 0.03164789080619812,\n",
       " -0.003304506652057171,\n",
       " 0.004641871899366379,\n",
       " 0.03758938983082771,\n",
       " -0.05924461781978607,\n",
       " 0.007028323598206043,\n",
       " 0.0038087160792201757,\n",
       " -0.025788908824324608,\n",
       " -0.021203335374593735,\n",
       " 0.022691207006573677,\n",
       " -0.021772969514131546,\n",
       " -0.2796376943588257,\n",
       " 0.0072674197144806385,\n",
       " 0.021072018891572952,\n",
       " 0.04519744589924812,\n",
       " -0.020534459501504898,\n",
       " 0.024313680827617645,\n",
       " -0.0006136745796538889,\n",
       " -0.011857046745717525,\n",
       " -0.03296777233481407,\n",
       " 0.035843122750520706,\n",
       " 0.031281668692827225,\n",
       " 0.06373956054449081,\n",
       " 0.04654785618185997,\n",
       " -0.014470553025603294,\n",
       " 0.015869708731770515,\n",
       " 0.033971283584833145,\n",
       " 0.01805952750146389,\n",
       " 0.0022987606935203075,\n",
       " 0.016549888998270035,\n",
       " -0.021714871749281883,\n",
       " -0.03486001864075661,\n",
       " -0.0008649316732771695,\n",
       " 0.15126045048236847,\n",
       " -0.02453671023249626,\n",
       " 0.030671194195747375,\n",
       " -0.007318212650716305,\n",
       " -0.006135467439889908,\n",
       " 0.06415148079395294,\n",
       " 0.0160214826464653,\n",
       " -0.03636447340250015,\n",
       " 0.01989860087633133,\n",
       " -0.021172357723116875,\n",
       " 0.04829411953687668,\n",
       " -0.04478100314736366,\n",
       " 0.047633860260248184,\n",
       " 0.0007749039214104414,\n",
       " -0.00592796178534627,\n",
       " 0.061542678624391556,\n",
       " 0.023968419060111046,\n",
       " 0.013305029831826687,\n",
       " 0.0226844884455204,\n",
       " 0.014538087882101536,\n",
       " -0.052159082144498825,\n",
       " -0.03274966776371002,\n",
       " 0.08583343774080276,\n",
       " -0.0037248542066663504,\n",
       " 0.0013494092272594571,\n",
       " 0.04091992974281311,\n",
       " 0.011659677140414715,\n",
       " 0.05843620002269745,\n",
       " -0.022286171093583107,\n",
       " -0.011520683765411377,\n",
       " 0.004705740138888359,\n",
       " 0.04718261957168579,\n",
       " -0.0019179153023287654,\n",
       " 0.0330093652009964,\n",
       " -0.03505057841539383,\n",
       " -0.020736563950777054,\n",
       " -0.009222183376550674,\n",
       " 0.014618286862969398,\n",
       " 0.006456066854298115,\n",
       " 0.0010978624923154712,\n",
       " 0.010223976336419582,\n",
       " 0.08537206798791885,\n",
       " 0.03883956000208855]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
