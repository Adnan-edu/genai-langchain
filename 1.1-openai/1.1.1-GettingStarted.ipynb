{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting started With Langchain And Open AI\n",
    "\n",
    "In this quickstart we'll see how to:\n",
    "\n",
    "- Get setup with LangChain, LangSmith and LangServe\n",
    "- Use the most basic and common components of LangChain: prompt templates, models, and output parsers.\n",
    "- Build a simple application with LangChain\n",
    "- Trace your application with LangSmith\n",
    "- Serve your application with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.Completions object at 0x1248b69e0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x124d10b20> root_client=<openai.OpenAI object at 0x105f7c040> root_async_client=<openai.AsyncOpenAI object at 0x1248b6a40> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input and get response form LLM\n",
    "\n",
    "result=llm.invoke(\"What is generative AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Generative AI refers to a subset of artificial intelligence that focuses on creating new content, such as text, images, music, or even code, based on patterns and data it has been trained on. Unlike traditional AI, which typically analyzes or classifies existing data, generative AI produces novel outputs that can resemble human creativity.\\n\\nKey technologies underlying generative AI include:\\n\\n1. **Generative Adversarial Networks (GANs):** These consist of two neural networks, a generator and a discriminator, that work together to produce realistic data. The generator creates new data, while the discriminator evaluates it against real data, guiding the generator to improve over time.\\n\\n2. **Variational Autoencoders (VAEs):** These are used to generate new data that is similar to the training data. They work by encoding input data into a lower-dimensional representation and then decoding it back to the original format, allowing for the creation of new, similar data points.\\n\\n3. **Transformers:** These are especially prominent in natural language processing (NLP) tasks. Models like GPT-3 (Generative Pre-trained Transformer 3) use large-scale transformers to generate human-like text based on the input they receive.\\n\\nApplications of generative AI include:\\n\\n- **Text Generation:** Creating articles, stories, and dialogue (e.g., chatbots, automated news writing).\\n- **Image Generation:** Producing realistic images, art, or even deepfakes.\\n- **Music Composition:** Composing new music tracks in various styles.\\n- **Code Generation:** Writing code snippets or entire programs based on given requirements.\\n\\nGenerative AI has immense potential but also raises ethical and societal concerns, such as the creation of misleading or harmful content, intellectual property issues, and the need for transparency and accountability in AI-generated outputs.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 359, 'prompt_tokens': 13, 'total_tokens': 372, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_e375328146', 'finish_reason': 'stop', 'logprobs': None} id='run-8f9b94eb-7562-4617-a58c-4b8054af2695-0' usage_metadata={'input_tokens': 13, 'output_tokens': 359, 'total_tokens': 372}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Chatprompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answers based on the questions\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    "\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Certainly! As of my latest update in October 2023, there isn\\'t a well-known tool or concept specifically called \"Langsmith\" in the field of AI, NLP, or software engineering. It is possible that \"Langsmith\" could refer to a new tool, framework, company, or concept that has emerged recently or is niche and not widely recognized.\\n\\nHowever, if you provide more context or specify the domain or industry you are referring to, I might be able to give a more accurate or relevant explanation.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 103, 'prompt_tokens': 33, 'total_tokens': 136, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_e375328146', 'finish_reason': 'stop', 'logprobs': None} id='run-7b99e9b8-274c-49a7-a1e2-bdfb61a9d5e1-0' usage_metadata={'input_tokens': 33, 'output_tokens': 103, 'total_tokens': 136}\n"
     ]
    }
   ],
   "source": [
    "## chain \n",
    "chain=prompt|llm\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Langsmith is a product developed by LangChain, designed to enhance the debugging and testing processes for applications that employ large language models (LLMs). Here are some of its key features and benefits:\n",
      "\n",
      "1. **Tracing and Observability**: Langsmith allows for detailed tracing of all LLM calls within an application. This feature helps developers understand the sequence of operations and interactions that led to a particular output. The visualizations and trace data can be instrumental in diagnosing issues and improving model performance.\n",
      "\n",
      "2. **Dataset Management**: Langsmith supports the creation and management of datasets that can be used to evaluate and benchmark LLMs. This includes tools for uploading, versioning, and organizing datasets, which are essential for consistent and repeatable testing.\n",
      "\n",
      "3. **Feedback Collection**: The platform facilitates the collection of feedback on model outputs. This feedback can be used to fine-tune models and improve their performance over time. By integrating user feedback directly into the testing loop, Langsmith helps create a more iterative and responsive development process.\n",
      "\n",
      "4. **Evaluation and Metrics**: Langsmith provides built-in tools for evaluating the performance of LLMs against various metrics. This includes both quantitative metrics (like accuracy and response time) and qualitative assessments (like coherence and relevance). These evaluations help developers identify strengths and weaknesses in their models.\n",
      "\n",
      "5. **Integration with LangChain**: As a product of LangChain, Langsmith integrates seamlessly with other LangChain tools and libraries. This allows for a unified workflow where you can develop, test, and deploy LLM-based applications using a cohesive set of tools.\n",
      "\n",
      "6. **User-Friendly Interface**: Langsmith features a user-friendly interface that makes it accessible to both novice and experienced developers. The intuitive design helps streamline the debugging and testing processes, making it easier to get actionable insights quickly.\n",
      "\n",
      "In summary, Langsmith is a comprehensive tool designed to improve the reliability and performance of applications using large language models by providing robust debugging, testing, and evaluation capabilities.\n"
     ]
    }
   ],
   "source": [
    "## stroutput Parser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt|llm|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
